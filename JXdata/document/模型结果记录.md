```
nohup python /home/chiyao/projects/Chinese-Text-Classification-Pytorch/run.py --model TextRCNN >> /home/chiyao/projects/Chinese-Text-Classification-Pytorch/JXdata/train_output.log 2>&1 &
tail -f /home/chiyao/projects/Chinese-Text-Classification-Pytorch/JXdata/train_output.log
```



## HANConcat[crime + judge + performance 并联]

```
"""
crime + judge + performance 并联
无 dynamic attention 版本
"""


class Config(object):

    """配置参数"""
    def __init__(self, dataset, embedding):
        self.model_name = 'HANConcat'

        self.train_path = dataset + '/data/train.csv'                                # 训练集
        self.dev_path = dataset + '/data/dev.csv'                                    # 验证集
        self.test_path = dataset + '/data/test.csv'                                  # 测试集
        self.class_list = [x.strip() for x in open(
            dataset + '/data/class.txt', encoding='utf-8').readlines()]              # 类别名单
        self.material_path = dataset + '/material/'                                  # 数据材料文件夹（停用词，词典）

        self.vocab_path = dataset + '/data/vocab_han.pkl'                                # 词表
        self.save_path = dataset + '/saved_dict/' + self.model_name + '.ckpt'        # 模型训练结果
        self.log_path = dataset + '/log/' + self.model_name
        self.embedding_pretrained = torch.tensor(
            np.load(dataset + '/data/' + embedding)["embeddings"].astype('float32'))\
            if embedding != 'random' else None                                       # 预训练词向量
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')   # 设备

        self.dropout = 0.5                                              # 随机失活
        self.require_improvement = 1000                                 # 若超过1000batch效果还没提升，则提前结束训练
        self.num_classes = len(self.class_list)                         # 类别数

        self.crime_n_vocab = 0                                          # crime词表大小，在运行时赋值
        self.judge_n_vocab = 0                                          # judge词表大小，在运行时赋值
        self.performance_n_vocab = 0                                    # performance词表大小，在运行时赋值
        self.fact_n_vocab = 0                                           # fact词表大小，在运行时赋值

        self.crime_max_length = 0                                       # crime取最大数，在运行时赋值
        self.judge_max_length = (0, 0)                                  # judge取最大的词、句数，在运行时赋值
        self.performance_max_length = (0, 0)                            # performance取最大的词、句数，在运行时赋值
        self.fact_max_length = (0, 0)                                   # fac取最大的词、句数，在运行时赋值

        self.embed = self.embedding_pretrained.size(1)\
            if self.embedding_pretrained is not None else 300           # 字向量维度, 若使用了预训练词向量，则维度统一

        self.crime_hidden_size = 128                                    # crime gru隐藏层
        self.judge_word_hidden_size = 128                               # judge gru隐藏层
        self.judge_sent_hidden_size = 128                               # judge gru隐藏层
        self.performance_word_hidden_size = 128                         # performance gru隐藏层
        self.performance_sent_hidden_size = 128                         # performance gru隐藏层
        self.fact_word_hidden_size = 128                                # fact gru隐藏层
        self.fact_sent_hidden_size = 128                                # fact gru隐藏层

        self.num_epochs = 100                                           # epoch数
        self.batch_size = 128                                           # mini-batch大小
        self.learning_rate = 1e-3                                       # 学习率
```

```
ssh://chiyao@10.176.37.201:22/home/chiyao/anaconda3/envs/han/bin/python -u /home/chiyao/projects/Chinese-Text-Classification-Pytorch/run.py --model HANConcat
Loading data...
Vocab size: crime: 399, judge: 5427, performance: 7171, fact: 10002
crime_max_length 3
judge_max_length (5, 10)
performance_max_length (6, 10)
fact_max_length (8, 33)
17058it [00:10, 1645.68it/s]
1895it [00:01, 1707.91it/s]
1895it [00:01, 1712.83it/s]
train dataset len: 17058
dev dataset len: 1895
test dataset len: 1895
Time usage: 0:00:16
<bound method Module.parameters of Model(
  (crime_encoder): WordAttNet(
    (lookup): Embedding(399, 300)
    (gru): GRU(300, 128, bidirectional=True)
  )
  (judge_encoder): HierAttNet(
    (word_att_net): WordAttNet(
      (lookup): Embedding(5427, 300)
      (gru): GRU(300, 128, bidirectional=True)
    )
    (sent_att_net): SentAttNet(
      (gru): GRU(256, 128, bidirectional=True)
    )
  )
  (performance_encoder): HierAttNet(
    (word_att_net): WordAttNet(
      (lookup): Embedding(7171, 300)
      (gru): GRU(300, 128, bidirectional=True)
    )
    (sent_att_net): SentAttNet(
      (gru): GRU(256, 128, bidirectional=True)
    )
  )
  (fc): Linear(in_features=768, out_features=4, bias=True)
  (dropout): Dropout(p=0.5, inplace=False)
)>
Epoch [1/100]
Iter:      0,  Train Loss:   1.5,  Train Acc: 17.19%,  Val Loss:   1.4,  Val Acc: 32.03%,  Time: 0:00:01 * 
Iter:    100,  Train Loss:  0.87,  Train Acc: 63.28%,  Val Loss:  0.89,  Val Acc: 63.85%,  Time: 0:00:16 * 
Epoch [2/100]
Iter:    200,  Train Loss:   0.7,  Train Acc: 70.31%,  Val Loss:  0.78,  Val Acc: 67.23%,  Time: 0:00:31 * 
Epoch [3/100]
Iter:    300,  Train Loss:  0.67,  Train Acc: 75.78%,  Val Loss:  0.73,  Val Acc: 70.24%,  Time: 0:00:46 * 
Iter:    400,  Train Loss:  0.53,  Train Acc: 80.47%,  Val Loss:  0.72,  Val Acc: 71.03%,  Time: 0:01:01 * 
Epoch [4/100]
Iter:    500,  Train Loss:  0.51,  Train Acc: 79.69%,  Val Loss:  0.75,  Val Acc: 68.23%,  Time: 0:01:16  
Epoch [5/100]
Iter:    600,  Train Loss:  0.55,  Train Acc: 78.12%,  Val Loss:  0.75,  Val Acc: 70.18%,  Time: 0:01:31  
Epoch [6/100]
Iter:    700,  Train Loss:  0.71,  Train Acc: 72.66%,  Val Loss:  0.76,  Val Acc: 70.77%,  Time: 0:01:46  
Iter:    800,  Train Loss:  0.49,  Train Acc: 80.47%,  Val Loss:  0.77,  Val Acc: 70.08%,  Time: 0:02:01  
Epoch [7/100]
Iter:    900,  Train Loss:  0.48,  Train Acc: 80.47%,  Val Loss:  0.79,  Val Acc: 69.23%,  Time: 0:02:16  
Epoch [8/100]
Iter:   1000,  Train Loss:   0.6,  Train Acc: 69.53%,  Val Loss:  0.78,  Val Acc: 69.18%,  Time: 0:02:31  
Epoch [9/100]
Iter:   1100,  Train Loss:  0.37,  Train Acc: 85.94%,  Val Loss:  0.82,  Val Acc: 71.03%,  Time: 0:02:46  
Iter:   1200,  Train Loss:   0.6,  Train Acc: 75.00%,  Val Loss:   0.8,  Val Acc: 70.87%,  Time: 0:03:01  
Epoch [10/100]
Iter:   1300,  Train Loss:  0.45,  Train Acc: 82.81%,  Val Loss:  0.82,  Val Acc: 70.45%,  Time: 0:03:16  
Epoch [11/100]
Iter:   1400,  Train Loss:  0.46,  Train Acc: 82.81%,  Val Loss:  0.84,  Val Acc: 70.24%,  Time: 0:03:31  
No optimization for a long time, auto-stopping...
Test Loss:  0.73,  Test Acc: 70.66%
Precision, Recall and F1-Score...
                         precision    recall  f1-score   support

             低（减刑0-3个月）     0.6863    0.4375    0.5344       240
             中（减刑4-7个月）     0.7293    0.8577    0.7883      1068
            高（减刑8-12个月）     0.6411    0.6204    0.6306       432
特殊（减去余下、减为无期、减为有期、不予减刑）     0.7353    0.3226    0.4484       155

               accuracy                         0.7066      1895
              macro avg     0.6980    0.5595    0.6004      1895
           weighted avg     0.7042    0.7066    0.6924      1895

Confusion Matrix...
[[105 119  10   6]
 [ 31 916 112   9]
 [  3 158 268   3]
 [ 14  63  28  50]]
Time usage: 0:00:01

Process finished with exit code 0

```

## HANConcat[crime + judge + performance + fact 并联]

```
"""
crime + judge + performance + fact 并联
无 dynamic attention 版本
"""


class Config(object):

    """配置参数"""
    def __init__(self, dataset, embedding):
        self.model_name = 'HANConcat'

        self.train_path = dataset + '/data/train.csv'                                # 训练集
        self.dev_path = dataset + '/data/dev.csv'                                    # 验证集
        self.test_path = dataset + '/data/test.csv'                                  # 测试集
        self.class_list = [x.strip() for x in open(
            dataset + '/data/class.txt', encoding='utf-8').readlines()]              # 类别名单
        self.material_path = dataset + '/material/'                                  # 数据材料文件夹（停用词，词典）

        self.vocab_path = dataset + '/data/vocab_han.pkl'                                # 词表
        self.save_path = dataset + '/saved_dict/' + self.model_name + '.ckpt'        # 模型训练结果
        self.log_path = dataset + '/log/' + self.model_name
        self.embedding_pretrained = torch.tensor(
            np.load(dataset + '/data/' + embedding)["embeddings"].astype('float32'))\
            if embedding != 'random' else None                                       # 预训练词向量
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')   # 设备

        self.dropout = 0.5                                              # 随机失活
        self.require_improvement = 1000                                 # 若超过1000batch效果还没提升，则提前结束训练
        self.num_classes = len(self.class_list)                         # 类别数

        self.crime_n_vocab = 0                                          # crime词表大小，在运行时赋值
        self.judge_n_vocab = 0                                          # judge词表大小，在运行时赋值
        self.performance_n_vocab = 0                                    # performance词表大小，在运行时赋值
        self.fact_n_vocab = 0                                           # fact词表大小，在运行时赋值

        self.crime_max_length = 0                                       # crime取最大数，在运行时赋值
        self.judge_max_length = (0, 0)                                  # judge取最大的词、句数，在运行时赋值
        self.performance_max_length = (0, 0)                            # performance取最大的词、句数，在运行时赋值
        self.fact_max_length = (0, 0)                                   # fac取最大的词、句数，在运行时赋值

        self.embed = self.embedding_pretrained.size(1)\
            if self.embedding_pretrained is not None else 300           # 字向量维度, 若使用了预训练词向量，则维度统一

        self.crime_hidden_size = 128                                    # crime gru隐藏层
        self.judge_word_hidden_size = 128                               # judge gru隐藏层
        self.judge_sent_hidden_size = 128                               # judge gru隐藏层
        self.performance_word_hidden_size = 128                         # performance gru隐藏层
        self.performance_sent_hidden_size = 128                         # performance gru隐藏层
        self.fact_word_hidden_size = 128                                # fact gru隐藏层
        self.fact_sent_hidden_size = 128                                # fact gru隐藏层

        self.num_epochs = 100                                           # epoch数
        self.batch_size = 128                                           # mini-batch大小
        self.learning_rate = 1e-3                                       # 学习率
```

```
ssh://chiyao@10.176.37.201:22/home/chiyao/anaconda3/envs/han/bin/python -u /home/chiyao/projects/Chinese-Text-Classification-Pytorch/run.py --model HANConcat
Loading data...
Vocab size: crime: 399, judge: 5427, performance: 7171, fact: 10002
crime_max_length 3
judge_max_length (5, 10)
performance_max_length (6, 10)
fact_max_length (8, 33)
17058it [00:10, 1622.59it/s]
1895it [00:01, 1678.77it/s]
1895it [00:01, 1679.63it/s]
train dataset len: 17058
dev dataset len: 1895
test dataset len: 1895
Time usage: 0:00:16
<bound method Module.parameters of Model(
  (crime_encoder): WordAttNet(
    (lookup): Embedding(399, 300)
    (gru): GRU(300, 128, bidirectional=True)
  )
  (judge_encoder): HierAttNet(
    (word_att_net): WordAttNet(
      (lookup): Embedding(5427, 300)
      (gru): GRU(300, 128, bidirectional=True)
    )
    (sent_att_net): SentAttNet(
      (gru): GRU(256, 128, bidirectional=True)
    )
  )
  (performance_encoder): HierAttNet(
    (word_att_net): WordAttNet(
      (lookup): Embedding(7171, 300)
      (gru): GRU(300, 128, bidirectional=True)
    )
    (sent_att_net): SentAttNet(
      (gru): GRU(256, 128, bidirectional=True)
    )
  )
  (fact_encoder): HierAttNet(
    (word_att_net): WordAttNet(
      (lookup): Embedding(10002, 300)
      (gru): GRU(300, 128, bidirectional=True)
    )
    (sent_att_net): SentAttNet(
      (gru): GRU(256, 128, bidirectional=True)
    )
  )
  (fc): Linear(in_features=768, out_features=4, bias=True)
  (dropout): Dropout(p=0.5, inplace=False)
)>
Epoch [1/100]
Iter:      0,  Train Loss:   1.4,  Train Acc: 26.56%,  Val Loss:   1.4,  Val Acc: 34.67%,  Time: 0:00:03 * 
Iter:    100,  Train Loss:  0.88,  Train Acc: 61.72%,  Val Loss:  0.91,  Val Acc: 63.22%,  Time: 0:00:29 * 
Epoch [2/100]
Iter:    200,  Train Loss:  0.72,  Train Acc: 71.09%,  Val Loss:   0.8,  Val Acc: 67.70%,  Time: 0:00:55 * 
Epoch [3/100]
Iter:    300,  Train Loss:  0.68,  Train Acc: 69.53%,  Val Loss:  0.75,  Val Acc: 68.18%,  Time: 0:01:19 * 
Iter:    400,  Train Loss:  0.57,  Train Acc: 78.12%,  Val Loss:  0.76,  Val Acc: 69.18%,  Time: 0:01:44  
Epoch [4/100]
Iter:    500,  Train Loss:  0.49,  Train Acc: 82.81%,  Val Loss:  0.76,  Val Acc: 68.71%,  Time: 0:02:09  
Epoch [5/100]
Iter:    600,  Train Loss:  0.57,  Train Acc: 78.12%,  Val Loss:  0.73,  Val Acc: 70.55%,  Time: 0:02:34 * 
Epoch [6/100]
Iter:    700,  Train Loss:  0.56,  Train Acc: 75.78%,  Val Loss:  0.74,  Val Acc: 70.55%,  Time: 0:02:58  
Iter:    800,  Train Loss:   0.5,  Train Acc: 79.69%,  Val Loss:  0.79,  Val Acc: 70.82%,  Time: 0:03:23  
Epoch [7/100]
Iter:    900,  Train Loss:  0.44,  Train Acc: 78.12%,  Val Loss:  0.77,  Val Acc: 70.66%,  Time: 0:03:51  
Epoch [8/100]
Iter:   1000,  Train Loss:  0.56,  Train Acc: 74.22%,  Val Loss:  0.78,  Val Acc: 69.23%,  Time: 0:04:16  
Epoch [9/100]
Iter:   1100,  Train Loss:  0.36,  Train Acc: 88.28%,  Val Loss:  0.82,  Val Acc: 70.82%,  Time: 0:04:41  
Iter:   1200,  Train Loss:  0.52,  Train Acc: 76.56%,  Val Loss:   0.8,  Val Acc: 70.45%,  Time: 0:05:05  
Epoch [10/100]
Iter:   1300,  Train Loss:  0.42,  Train Acc: 82.03%,  Val Loss:  0.83,  Val Acc: 69.71%,  Time: 0:05:29  
Epoch [11/100]
Iter:   1400,  Train Loss:  0.47,  Train Acc: 82.03%,  Val Loss:  0.84,  Val Acc: 70.66%,  Time: 0:05:53  
Epoch [12/100]
Iter:   1500,  Train Loss:  0.35,  Train Acc: 85.16%,  Val Loss:  0.85,  Val Acc: 71.66%,  Time: 0:06:17  
Iter:   1600,  Train Loss:  0.32,  Train Acc: 88.28%,  Val Loss:  0.87,  Val Acc: 70.03%,  Time: 0:06:40  
No optimization for a long time, auto-stopping...
Test Loss:  0.72,  Test Acc: 71.03%
Precision, Recall and F1-Score...
                         precision    recall  f1-score   support

             低（减刑0-3个月）     0.6212    0.5125    0.5616       240
             中（减刑4-7个月）     0.7589    0.8193    0.7879      1068
            高（减刑8-12个月）     0.6477    0.6597    0.6537       432
特殊（减去余下、减为无期、减为有期、不予减刑）     0.6058    0.4065    0.4865       155

               accuracy                         0.7103      1895
              macro avg     0.6584    0.5995    0.6224      1895
           weighted avg     0.7036    0.7103    0.7040      1895

Confusion Matrix...
[[123  97  10  10]
 [ 52 875 123  18]
 [  8 126 285  13]
 [ 15  55  22  63]]
Time usage: 0:00:02

Process finished with exit code 0

```

## HANConcat_Dynamic[crime + judge + performance + fact(<-crime) 并联]

```
"""
crime + judge + performance + fact(<-crime) 并联
有 dynamic attention 版本
"""


class Config(object):

    """配置参数"""
    def __init__(self, dataset, embedding):
        self.model_name = 'HANConcat_Dynamic'

        self.train_path = dataset + '/data/train.csv'                                # 训练集
        self.dev_path = dataset + '/data/dev.csv'                                    # 验证集
        self.test_path = dataset + '/data/test.csv'                                  # 测试集
        self.class_list = [x.strip() for x in open(
            dataset + '/data/class.txt', encoding='utf-8').readlines()]              # 类别名单
        self.material_path = dataset + '/material/'                                  # 数据材料文件夹（停用词，词典）

        self.vocab_path = dataset + '/data/vocab_han.pkl'                                # 词表
        self.save_path = dataset + '/saved_dict/' + self.model_name + '.ckpt'        # 模型训练结果
        self.log_path = dataset + '/log/' + self.model_name
        self.embedding_pretrained = torch.tensor(
            np.load(dataset + '/data/' + embedding)["embeddings"].astype('float32'))\
            if embedding != 'random' else None                                       # 预训练词向量
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')   # 设备

        self.dropout = 0.5                                              # 随机失活
        self.require_improvement = 1000                                 # 若超过1000batch效果还没提升，则提前结束训练
        self.num_classes = len(self.class_list)                         # 类别数

        self.crime_n_vocab = 0                                          # crime词表大小，在运行时赋值
        self.judge_n_vocab = 0                                          # judge词表大小，在运行时赋值
        self.performance_n_vocab = 0                                    # performance词表大小，在运行时赋值
        self.fact_n_vocab = 0                                           # fact词表大小，在运行时赋值

        self.crime_max_length = 0                                       # crime取最大数，在运行时赋值
        self.judge_max_length = (0, 0)                                  # judge取最大的词、句数，在运行时赋值
        self.performance_max_length = (0, 0)                            # performance取最大的词、句数，在运行时赋值
        self.fact_max_length = (0, 0)                                   # fac取最大的词、句数，在运行时赋值

        self.embed = self.embedding_pretrained.size(1)\
            if self.embedding_pretrained is not None else 300           # 字向量维度, 若使用了预训练词向量，则维度统一

        self.crime_hidden_size = 128                                    # crime gru隐藏层
        self.judge_word_hidden_size = 128                               # judge gru隐藏层
        self.judge_sent_hidden_size = 128                               # judge gru隐藏层
        self.performance_word_hidden_size = 128                         # performance gru隐藏层
        self.performance_sent_hidden_size = 128                         # performance gru隐藏层
        self.fact_word_hidden_size = 128                                # fact gru隐藏层
        self.fact_sent_hidden_size = 128                                # fact gru隐藏层

        self.num_epochs = 100                                           # epoch数
        self.batch_size = 128                                           # mini-batch大小
        self.learning_rate = 1e-3                                       # 学习率

```

```
ssh://chiyao@10.176.37.201:22/home/chiyao/anaconda3/envs/han/bin/python -u /home/chiyao/projects/Chinese-Text-Classification-Pytorch/run.py --model HANConcat_Dynamic
Loading data...
Vocab size: crime: 399, judge: 5427, performance: 7171, fact: 10002
crime_max_length 3
judge_max_length (5, 10)
performance_max_length (6, 10)
fact_max_length (8, 33)
17058it [00:10, 1657.12it/s]
1895it [00:01, 1714.15it/s]
1895it [00:01, 1718.14it/s]
train dataset len: 17058
dev dataset len: 1895
test dataset len: 1895
Time usage: 0:00:16
<bound method Module.parameters of Model(
  (crime_encoder): WordAttNet(
    (lookup): Embedding(399, 300)
    (gru): GRU(300, 128, bidirectional=True)
  )
  (judge_encoder): HierAttNet(
    (word_att_net): WordAttNet(
      (lookup): Embedding(5427, 300)
      (gru): GRU(300, 128, bidirectional=True)
    )
    (sent_att_net): SentAttNet(
      (gru): GRU(256, 128, bidirectional=True)
    )
  )
  (performance_encoder): HierAttNet(
    (word_att_net): WordAttNet(
      (lookup): Embedding(7171, 300)
      (gru): GRU(300, 128, bidirectional=True)
    )
    (sent_att_net): SentAttNet(
      (gru): GRU(256, 128, bidirectional=True)
    )
  )
  (fact_encoder): HierAttDynamicNet(
    (word_att_net): WordAttDynamicNet(
      (lookup): Embedding(10002, 300)
      (gru): GRU(300, 128, bidirectional=True)
    )
    (sent_att_net): SentAttDynamicNet(
      (gru): GRU(256, 128, bidirectional=True)
    )
  )
  (fc): Linear(in_features=1024, out_features=4, bias=True)
  (dropout): Dropout(p=0.5, inplace=False)
)>
Epoch [1/100]
Iter:      0,  Train Loss:   1.5,  Train Acc: 24.22%,  Val Loss:   1.3,  Val Acc: 38.42%,  Time: 0:00:02 * 
Iter:    100,  Train Loss:  0.88,  Train Acc: 63.28%,  Val Loss:   0.9,  Val Acc: 63.11%,  Time: 0:00:40 * 
Epoch [2/100]
Iter:    200,  Train Loss:  0.74,  Train Acc: 71.88%,  Val Loss:  0.78,  Val Acc: 67.23%,  Time: 0:01:17 * 
Epoch [3/100]
Iter:    300,  Train Loss:  0.66,  Train Acc: 75.78%,  Val Loss:  0.74,  Val Acc: 69.82%,  Time: 0:01:55 * 
Iter:    400,  Train Loss:  0.57,  Train Acc: 75.78%,  Val Loss:  0.75,  Val Acc: 70.40%,  Time: 0:02:32  
Epoch [4/100]
Iter:    500,  Train Loss:  0.51,  Train Acc: 78.91%,  Val Loss:  0.79,  Val Acc: 68.28%,  Time: 0:03:09  
Epoch [5/100]
Iter:    600,  Train Loss:  0.49,  Train Acc: 81.25%,  Val Loss:   0.8,  Val Acc: 68.23%,  Time: 0:03:47  
Epoch [6/100]
Iter:    700,  Train Loss:  0.63,  Train Acc: 76.56%,  Val Loss:  0.89,  Val Acc: 65.86%,  Time: 0:04:24  
Iter:    800,  Train Loss:  0.45,  Train Acc: 82.03%,  Val Loss:  0.87,  Val Acc: 68.65%,  Time: 0:05:01  
Epoch [7/100]
Iter:    900,  Train Loss:  0.39,  Train Acc: 83.59%,  Val Loss:  0.93,  Val Acc: 67.18%,  Time: 0:05:38  
Epoch [8/100]
Iter:   1000,  Train Loss:  0.36,  Train Acc: 86.72%,  Val Loss:  0.95,  Val Acc: 66.23%,  Time: 0:06:15  
Epoch [9/100]
Iter:   1100,  Train Loss:  0.28,  Train Acc: 88.28%,  Val Loss:  0.98,  Val Acc: 67.97%,  Time: 0:06:52  
Iter:   1200,  Train Loss:  0.37,  Train Acc: 85.16%,  Val Loss:   1.0,  Val Acc: 67.55%,  Time: 0:07:30  
Epoch [10/100]
Iter:   1300,  Train Loss:  0.34,  Train Acc: 87.50%,  Val Loss:   1.0,  Val Acc: 67.02%,  Time: 0:08:08  
No optimization for a long time, auto-stopping...
Test Loss:  0.75,  Test Acc: 69.39%
Precision, Recall and F1-Score...
                         precision    recall  f1-score   support

             低（减刑0-3个月）     0.7541    0.3833    0.5083       240
             中（减刑4-7个月）     0.7223    0.8427    0.7779      1068
            高（减刑8-12个月）     0.6387    0.5810    0.6085       432
特殊（减去余下、减为无期、减为有期、不予减刑）     0.5373    0.4645    0.4983       155

               accuracy                         0.6939      1895
              macro avg     0.6631    0.5679    0.5982      1895
           weighted avg     0.6921    0.6939    0.6822      1895

Confusion Matrix...
[[ 92 125  11  12]
 [ 21 900 108  39]
 [  1 169 251  11]
 [  8  52  23  72]]
Time usage: 0:00:02

Process finished with exit code 0

```

学习率指数衰减

```
ssh://chiyao@10.176.37.201:22/home/chiyao/anaconda3/envs/han/bin/python -u /home/chiyao/projects/Chinese-Text-Classification-Pytorch/run.py --model HANConcat_Dynamic
Loading data...
Vocab size: crime: 399, judge: 5427, performance: 7171, fact: 10002
crime_max_length 3
judge_max_length (5, 10)
performance_max_length (6, 10)
fact_max_length (8, 33)
17058it [00:10, 1642.49it/s]
1895it [00:01, 1678.04it/s]
1895it [00:01, 1692.77it/s]
train dataset len: 17058
dev dataset len: 1895
test dataset len: 1895
Time usage: 0:00:16
<bound method Module.parameters of Model(
  (crime_encoder): WordAttNet(
    (lookup): Embedding(399, 300)
    (gru): GRU(300, 128, bidirectional=True)
  )
  (judge_encoder): HierAttNet(
    (word_att_net): WordAttNet(
      (lookup): Embedding(5427, 300)
      (gru): GRU(300, 128, bidirectional=True)
    )
    (sent_att_net): SentAttNet(
      (gru): GRU(256, 128, bidirectional=True)
    )
  )
  (performance_encoder): HierAttNet(
    (word_att_net): WordAttNet(
      (lookup): Embedding(7171, 300)
      (gru): GRU(300, 128, bidirectional=True)
    )
    (sent_att_net): SentAttNet(
      (gru): GRU(256, 128, bidirectional=True)
    )
  )
  (fact_encoder): HierAttDynamicNet(
    (word_att_net): WordAttDynamicNet(
      (lookup): Embedding(10002, 300)
      (gru): GRU(300, 128, bidirectional=True)
    )
    (sent_att_net): SentAttDynamicNet(
      (gru): GRU(256, 128, bidirectional=True)
    )
  )
  (fc): Linear(in_features=1024, out_features=4, bias=True)
  (dropout): Dropout(p=0.5, inplace=False)
)>
Epoch [1/100]
/home/chiyao/anaconda3/envs/han/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Iter:      0,  Train Loss:   1.5,  Train Acc: 24.22%,  Val Loss:   1.3,  Val Acc: 37.26%,  Time: 0:00:03 * 
Iter:    100,  Train Loss:  0.87,  Train Acc: 64.84%,  Val Loss:  0.91,  Val Acc: 61.69%,  Time: 0:00:40 * 
Epoch [2/100]
Iter:    200,  Train Loss:  0.75,  Train Acc: 73.44%,  Val Loss:  0.79,  Val Acc: 66.86%,  Time: 0:01:19 * 
Epoch [3/100]
Iter:    300,  Train Loss:  0.65,  Train Acc: 75.78%,  Val Loss:  0.75,  Val Acc: 69.13%,  Time: 0:01:56 * 
Iter:    400,  Train Loss:  0.56,  Train Acc: 74.22%,  Val Loss:  0.74,  Val Acc: 70.34%,  Time: 0:02:34 * 
Epoch [4/100]
Iter:    500,  Train Loss:  0.47,  Train Acc: 82.03%,  Val Loss:  0.79,  Val Acc: 67.12%,  Time: 0:03:13  
Epoch [5/100]
Iter:    600,  Train Loss:  0.56,  Train Acc: 77.34%,  Val Loss:   0.8,  Val Acc: 68.44%,  Time: 0:03:50  
Epoch [6/100]
Iter:    700,  Train Loss:  0.57,  Train Acc: 77.34%,  Val Loss:  0.86,  Val Acc: 68.18%,  Time: 0:04:28  
Iter:    800,  Train Loss:  0.41,  Train Acc: 83.59%,  Val Loss:  0.86,  Val Acc: 68.92%,  Time: 0:05:06  
Epoch [7/100]
Iter:    900,  Train Loss:  0.37,  Train Acc: 81.25%,  Val Loss:  0.94,  Val Acc: 68.13%,  Time: 0:05:43  
Epoch [8/100]
Iter:   1000,  Train Loss:  0.35,  Train Acc: 82.03%,  Val Loss:  0.93,  Val Acc: 68.18%,  Time: 0:06:21  
Epoch [9/100]
Iter:   1100,  Train Loss:  0.24,  Train Acc: 89.06%,  Val Loss:  0.96,  Val Acc: 68.02%,  Time: 0:06:58  
Iter:   1200,  Train Loss:  0.35,  Train Acc: 85.94%,  Val Loss:   1.0,  Val Acc: 67.18%,  Time: 0:07:36  
Epoch [10/100]
Iter:   1300,  Train Loss:  0.29,  Train Acc: 89.06%,  Val Loss:   1.1,  Val Acc: 66.91%,  Time: 0:08:13  
Epoch [11/100]
Iter:   1400,  Train Loss:  0.24,  Train Acc: 90.62%,  Val Loss:   1.0,  Val Acc: 67.39%,  Time: 0:08:51  
No optimization for a long time, auto-stopping...
Test Loss:  0.74,  Test Acc: 69.23%
Precision, Recall and F1-Score...
                         precision    recall  f1-score   support

             低（减刑0-3个月）     0.7203    0.4292    0.5379       240
             中（减刑4-7个月）     0.7152    0.8464    0.7753      1068
            高（减刑8-12个月）     0.6154    0.5926    0.6038       432
特殊（减去余下、减为无期、减为有期、不予减刑）     0.6806    0.3161    0.4317       155

               accuracy                         0.6923      1895
              macro avg     0.6829    0.5461    0.5872      1895
           weighted avg     0.6902    0.6923    0.6780      1895

Confusion Matrix...
[[103 118  12   7]
 [ 27 904 125  12]
 [  2 170 256   4]
 [ 11  72  23  49]]
Time usage: 0:00:02

Process finished with exit code 0

```

```
ssh://chiyao@10.176.37.201:22/home/chiyao/anaconda3/envs/han/bin/python -u /home/chiyao/projects/Chinese-Text-Classification-Pytorch/run.py --model HANConcat_Dynamic
Loading data...
Vocab size: crime: 399, judge: 5427, performance: 7171, fact: 10002
crime_max_length 3
judge_max_length (5, 10)
performance_max_length (6, 10)
fact_max_length (8, 33)
17058it [00:10, 1651.88it/s]
1895it [00:01, 1706.22it/s]
1895it [00:01, 1715.24it/s]
train dataset len: 17058
dev dataset len: 1895
test dataset len: 1895
Time usage: 0:00:16
<bound method Module.parameters of Model(
  (crime_encoder): WordAttNet(
    (lookup): Embedding(399, 300)
    (gru): GRU(300, 128, bidirectional=True)
  )
  (judge_encoder): HierAttNet(
    (word_att_net): WordAttNet(
      (lookup): Embedding(5427, 300)
      (gru): GRU(300, 128, bidirectional=True)
    )
    (sent_att_net): SentAttNet(
      (gru): GRU(256, 128, bidirectional=True)
    )
  )
  (performance_encoder): HierAttNet(
    (word_att_net): WordAttNet(
      (lookup): Embedding(7171, 300)
      (gru): GRU(300, 128, bidirectional=True)
    )
    (sent_att_net): SentAttNet(
      (gru): GRU(256, 128, bidirectional=True)
    )
  )
  (fact_encoder): HierAttDynamicNet(
    (word_att_net): WordAttDynamicNet(
      (lookup): Embedding(10002, 300)
      (gru): GRU(300, 128, bidirectional=True)
    )
    (sent_att_net): SentAttDynamicNet(
      (gru): GRU(256, 128, bidirectional=True)
    )
  )
  (fc): Linear(in_features=1024, out_features=4, bias=True)
  (dropout): Dropout(p=0.5, inplace=False)
)>
Epoch [1/100]
Iter:      0,  Train Loss:   1.5,  Train Acc: 24.22%,  Val Loss:   1.3,  Val Acc: 38.42%,  Time: 0:00:02 * 
Iter:    100,  Train Loss:  0.88,  Train Acc: 63.28%,  Val Loss:   0.9,  Val Acc: 63.11%,  Time: 0:00:42 * 
Epoch [2/100]
Iter:    200,  Train Loss:  0.74,  Train Acc: 73.44%,  Val Loss:  0.78,  Val Acc: 67.39%,  Time: 0:01:21 * 
Epoch [3/100]
Iter:    300,  Train Loss:  0.62,  Train Acc: 77.34%,  Val Loss:  0.74,  Val Acc: 70.66%,  Time: 0:02:01 * 
Iter:    400,  Train Loss:  0.59,  Train Acc: 75.00%,  Val Loss:  0.74,  Val Acc: 70.03%,  Time: 0:02:41 * 
Epoch [4/100]
Iter:    500,  Train Loss:   0.5,  Train Acc: 78.91%,  Val Loss:   0.8,  Val Acc: 66.97%,  Time: 0:03:20  
Epoch [5/100]
Iter:    600,  Train Loss:  0.54,  Train Acc: 80.47%,  Val Loss:  0.81,  Val Acc: 69.18%,  Time: 0:03:59  
Epoch [6/100]
Iter:    700,  Train Loss:  0.57,  Train Acc: 82.03%,  Val Loss:  0.87,  Val Acc: 69.18%,  Time: 0:04:38  
Iter:    800,  Train Loss:  0.41,  Train Acc: 82.03%,  Val Loss:  0.88,  Val Acc: 68.60%,  Time: 0:05:17  
Epoch [7/100]
Iter:    900,  Train Loss:  0.37,  Train Acc: 83.59%,  Val Loss:  0.93,  Val Acc: 68.23%,  Time: 0:05:56  
Epoch [8/100]
Iter:   1000,  Train Loss:  0.34,  Train Acc: 85.16%,  Val Loss:  0.92,  Val Acc: 68.28%,  Time: 0:06:37  
Epoch [9/100]
Iter:   1100,  Train Loss:  0.24,  Train Acc: 91.41%,  Val Loss:   1.0,  Val Acc: 68.18%,  Time: 0:07:17  
Iter:   1200,  Train Loss:  0.37,  Train Acc: 87.50%,  Val Loss:   1.1,  Val Acc: 66.91%,  Time: 0:07:56  
Epoch [10/100]
Iter:   1300,  Train Loss:  0.32,  Train Acc: 88.28%,  Val Loss:   1.1,  Val Acc: 67.18%,  Time: 0:08:35  
Epoch [11/100]
Iter:   1400,  Train Loss:  0.23,  Train Acc: 91.41%,  Val Loss:   1.1,  Val Acc: 67.92%,  Time: 0:09:14  
No optimization for a long time, auto-stopping...
Test Loss:  0.75,  Test Acc: 70.29%
Precision, Recall and F1-Score...
                         precision    recall  f1-score   support

             低（减刑0-3个月）     0.7687    0.4292    0.5508       240
             中（减刑4-7个月）     0.7267    0.8464    0.7820      1068
            高（减刑8-12个月）     0.6221    0.6250    0.6236       432
特殊（减去余下、减为无期、减为有期、不予减刑）     0.6627    0.3548    0.4622       155

               accuracy                         0.7029      1895
              macro avg     0.6950    0.5639    0.6046      1895
           weighted avg     0.7029    0.7029    0.6904      1895

Confusion Matrix...
[[103 118  12   7]
 [ 22 904 128  14]
 [  3 152 270   7]
 [  6  70  24  55]]
Time usage: 0:00:02

Process finished with exit code 0

```

## TextCNN

```
class Config(object):

    """配置参数"""
    def __init__(self, dataset, embedding):
        self.model_name = 'TextRNN'
        self.train_path = dataset + '/data/train.txt'                                # 训练集
        self.dev_path = dataset + '/data/dev.txt'                                    # 验证集
        self.test_path = dataset + '/data/test.txt'                                  # 测试集
        self.class_list = [x.strip() for x in open(
            dataset + '/data/class.txt', encoding='utf-8').readlines()]              # 类别名单
        self.material_path = dataset + '/material/'                                  # 数据材料文件夹（停用词，词典）
        self.vocab_path = dataset + '/data/vocab.pkl'                                # 词表
        self.save_path = dataset + '/saved_dict/' + self.model_name + '.ckpt'        # 模型训练结果
        self.log_path = dataset + '/log/' + self.model_name
        self.embedding_pretrained = torch.tensor(
            np.load(dataset + '/data/' + embedding)["embeddings"].astype('float32'))\
            if embedding != 'random' else None                                       # 预训练词向量
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')   # 设备

        self.dropout = 0.5                                              # 随机失活
        self.require_improvement = 1000                                 # 若超过1000batch效果还没提升，则提前结束训练
        self.num_classes = len(self.class_list)                         # 类别数
        self.n_vocab = 0                                                # 词表大小，在运行时赋值
        self.num_epochs = 100                                           # epoch数
        self.batch_size = 128                                           # mini-batch大小
        self.pad_size = 32                                              # 每句话处理成的长度(短填长切)
        self.learning_rate = 1e-3                                       # 学习率
        self.embed = self.embedding_pretrained.size(1)\
            if self.embedding_pretrained is not None else 300           # 字向量维度, 若使用了预训练词向量，则维度统一
        self.hidden_size = 128                                          # lstm隐藏层
        self.num_layers = 2                                             # lstm层数

```

```
ssh://chiyao@10.176.37.201:22/home/chiyao/anaconda3/envs/han/bin/python -u /home/chiyao/projects/Chinese-Text-Classification-Pytorch/run.py --model TextCNN
Loading data...
Vocab size: 10002
17058it [00:02, 7359.83it/s]
1895it [00:00, 7645.72it/s]
1895it [00:00, 6095.45it/s]
Time usage: 0:00:03
<bound method Module.parameters of Model(
  (embedding): Embedding(10002, 300, padding_idx=0)
  (convs): ModuleList(
    (0): Conv2d(1, 256, kernel_size=(2, 300), stride=(1, 1))
    (1): Conv2d(1, 256, kernel_size=(3, 300), stride=(1, 1))
    (2): Conv2d(1, 256, kernel_size=(4, 300), stride=(1, 1))
  )
  (dropout): Dropout(p=0.5, inplace=False)
  (fc): Linear(in_features=768, out_features=4, bias=True)
)>
Epoch [1/100]
Iter:      0,  Train Loss:   1.5,  Train Acc: 21.88%,  Val Loss:   2.4,  Val Acc: 56.36%,  Time: 0:00:01 * 
Iter:    100,  Train Loss:   1.0,  Train Acc: 57.81%,  Val Loss:  0.93,  Val Acc: 61.48%,  Time: 0:00:03 * 
Epoch [2/100]
Iter:    200,  Train Loss:  0.93,  Train Acc: 67.97%,  Val Loss:  0.89,  Val Acc: 64.22%,  Time: 0:00:08 * 
Epoch [3/100]
Iter:    300,  Train Loss:  0.81,  Train Acc: 65.62%,  Val Loss:  0.87,  Val Acc: 64.49%,  Time: 0:00:14 * 
Iter:    400,  Train Loss:  0.86,  Train Acc: 64.06%,  Val Loss:  0.84,  Val Acc: 64.70%,  Time: 0:00:16 * 
Epoch [4/100]
Iter:    500,  Train Loss:  0.73,  Train Acc: 69.53%,  Val Loss:  0.84,  Val Acc: 64.80%,  Time: 0:00:22  
Epoch [5/100]
Iter:    600,  Train Loss:  0.71,  Train Acc: 67.19%,  Val Loss:  0.84,  Val Acc: 65.96%,  Time: 0:00:28 * 
Epoch [6/100]
Iter:    700,  Train Loss:  0.75,  Train Acc: 71.09%,  Val Loss:  0.83,  Val Acc: 65.17%,  Time: 0:00:34 * 
Iter:    800,  Train Loss:  0.62,  Train Acc: 76.56%,  Val Loss:  0.83,  Val Acc: 65.91%,  Time: 0:00:36 * 
Epoch [7/100]
Iter:    900,  Train Loss:  0.71,  Train Acc: 67.97%,  Val Loss:  0.84,  Val Acc: 65.91%,  Time: 0:00:41  
Epoch [8/100]
Iter:   1000,  Train Loss:  0.71,  Train Acc: 63.28%,  Val Loss:  0.84,  Val Acc: 66.33%,  Time: 0:00:47  
Epoch [9/100]
Iter:   1100,  Train Loss:  0.69,  Train Acc: 75.00%,  Val Loss:  0.84,  Val Acc: 65.96%,  Time: 0:00:53  
Iter:   1200,  Train Loss:   0.6,  Train Acc: 75.78%,  Val Loss:  0.85,  Val Acc: 65.91%,  Time: 0:00:55  
Epoch [10/100]
Iter:   1300,  Train Loss:  0.64,  Train Acc: 74.22%,  Val Loss:  0.85,  Val Acc: 65.28%,  Time: 0:01:01  
Epoch [11/100]
Iter:   1400,  Train Loss:  0.57,  Train Acc: 78.91%,  Val Loss:  0.86,  Val Acc: 66.54%,  Time: 0:01:07  
Epoch [12/100]
Iter:   1500,  Train Loss:  0.57,  Train Acc: 77.34%,  Val Loss:  0.86,  Val Acc: 65.91%,  Time: 0:01:12  
Iter:   1600,  Train Loss:  0.59,  Train Acc: 74.22%,  Val Loss:  0.87,  Val Acc: 66.07%,  Time: 0:01:14  
Epoch [13/100]
Iter:   1700,  Train Loss:  0.53,  Train Acc: 78.91%,  Val Loss:  0.87,  Val Acc: 66.33%,  Time: 0:01:20  
Epoch [14/100]
Iter:   1800,  Train Loss:  0.56,  Train Acc: 75.78%,  Val Loss:  0.87,  Val Acc: 66.39%,  Time: 0:01:26  
No optimization for a long time, auto-stopping...
Test Loss:  0.83,  Test Acc: 65.91%
Precision, Recall and F1-Score...
                        			 precision    recall  f1-score   support

             低（减刑0-3个月）     0.6500    0.4333    0.5200       240
             中（减刑4-7个月）     0.6977    0.8277    0.7572      1068
            高（减刑8-12个月）     0.5838    0.4838    0.5291       432
特殊														 0.4727    0.3355    0.3925       155

               accuracy                         0.6591      1895
              macro avg     0.6011    0.5201    0.5497      1895
           weighted avg     0.6473    0.6591    0.6453      1895

Confusion Matrix...
[[104 110  11  15]
 [ 38 884 117  29]
 [  6 203 209  14]
 [ 12  70  21  52]]
Time usage: 0:00:00

Process finished with exit code 0

```

## TextRNN

```
class Config(object):

    """配置参数"""
    def __init__(self, dataset, embedding):
        self.model_name = 'TextRNN'
        self.train_path = dataset + '/data/train.csv'                                # 训练集
        self.dev_path = dataset + '/data/dev.csv'                                    # 验证集
        self.test_path = dataset + '/data/test.csv'                                  # 测试集
        self.class_list = [x.strip() for x in open(
            dataset + '/data/class.txt', encoding='utf-8').readlines()]              # 类别名单
        self.material_path = dataset + '/material/'                                  # 数据材料文件夹（停用词，词典）
        self.vocab_path = dataset + '/data/vocab.pkl'                                # 词表
        self.save_path = dataset + '/saved_dict/' + self.model_name + '.ckpt'        # 模型训练结果
        self.log_path = dataset + '/log/' + self.model_name
        self.embedding_pretrained = torch.tensor(
            np.load(dataset + '/data/' + embedding)["embeddings"].astype('float32'))\
            if embedding != 'random' else None                                       # 预训练词向量
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')   # 设备

        self.dropout = 0.5                                              # 随机失活
        self.require_improvement = 1000                                 # 若超过1000batch效果还没提升，则提前结束训练
        self.num_classes = len(self.class_list)                         # 类别数
        self.n_vocab = 0                                                # 词表大小，在运行时赋值
        self.num_epochs = 100                                           # epoch数
        self.batch_size = 128                                           # mini-batch大小
        self.pad_size = 32                                              # 每句话处理成的长度(短填长切)
        self.learning_rate = 1e-3                                       # 学习率
        self.embed = self.embedding_pretrained.size(1)\
            if self.embedding_pretrained is not None else 300           # 字向量维度, 若使用了预训练词向量，则维度统一
        self.hidden_size = 128                                          # lstm隐藏层
        self.num_layers = 2                                             # lstm层数

```

```
ssh://chiyao@10.176.37.201:22/home/chiyao/anaconda3/envs/han/bin/python -u /home/chiyao/projects/Chinese-Text-Classification-Pytorch/run.py --model TextRNN
Loading data...
Vocab size: 10002
17058it [00:02, 7047.13it/s]
1895it [00:00, 7315.52it/s]
1895it [00:00, 5845.38it/s]
Time usage: 0:00:04
<bound method Module.parameters of Model(
  (embedding): Embedding(10002, 300, padding_idx=10001)
  (lstm): LSTM(300, 128, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)
  (fc): Linear(in_features=256, out_features=4, bias=True)
)>
Epoch [1/100]
Iter:      0,  Train Loss:   1.4,  Train Acc: 14.84%,  Val Loss:   1.4,  Val Acc: 45.44%,  Time: 0:00:00 * 
Iter:    100,  Train Loss:  0.93,  Train Acc: 60.16%,  Val Loss:   1.0,  Val Acc: 57.26%,  Time: 0:00:03 * 
Epoch [2/100]
Iter:    200,  Train Loss:  0.84,  Train Acc: 63.28%,  Val Loss:  0.92,  Val Acc: 62.22%,  Time: 0:00:06 * 
Epoch [3/100]
Iter:    300,  Train Loss:  0.77,  Train Acc: 69.53%,  Val Loss:   0.9,  Val Acc: 61.79%,  Time: 0:00:08 * 
Iter:    400,  Train Loss:  0.89,  Train Acc: 66.41%,  Val Loss:  0.88,  Val Acc: 64.17%,  Time: 0:00:11 * 
Epoch [4/100]
Iter:    500,  Train Loss:  0.69,  Train Acc: 67.97%,  Val Loss:  0.87,  Val Acc: 64.17%,  Time: 0:00:14 * 
Epoch [5/100]
Iter:    600,  Train Loss:  0.75,  Train Acc: 68.75%,  Val Loss:  0.87,  Val Acc: 65.07%,  Time: 0:00:17  
Epoch [6/100]
Iter:    700,  Train Loss:  0.77,  Train Acc: 71.09%,  Val Loss:  0.91,  Val Acc: 65.70%,  Time: 0:00:19  
Iter:    800,  Train Loss:  0.59,  Train Acc: 71.88%,  Val Loss:   0.9,  Val Acc: 65.86%,  Time: 0:00:22  
Epoch [7/100]
Iter:    900,  Train Loss:  0.61,  Train Acc: 71.88%,  Val Loss:  0.91,  Val Acc: 65.65%,  Time: 0:00:24  
Epoch [8/100]
Iter:   1000,  Train Loss:  0.76,  Train Acc: 68.75%,  Val Loss:  0.96,  Val Acc: 62.43%,  Time: 0:00:26  
Epoch [9/100]
Iter:   1100,  Train Loss:  0.61,  Train Acc: 75.78%,  Val Loss:  0.98,  Val Acc: 65.54%,  Time: 0:00:29  
Iter:   1200,  Train Loss:  0.56,  Train Acc: 80.47%,  Val Loss:  0.98,  Val Acc: 65.17%,  Time: 0:00:31  
Epoch [10/100]
Iter:   1300,  Train Loss:  0.59,  Train Acc: 78.12%,  Val Loss:   1.1,  Val Acc: 64.70%,  Time: 0:00:33  
Epoch [11/100]
Iter:   1400,  Train Loss:   0.5,  Train Acc: 81.25%,  Val Loss:  0.98,  Val Acc: 64.33%,  Time: 0:00:36  
Epoch [12/100]
Iter:   1500,  Train Loss:  0.51,  Train Acc: 75.78%,  Val Loss:   1.1,  Val Acc: 63.59%,  Time: 0:00:38  
No optimization for a long time, auto-stopping...
Test Loss:  0.87,  Test Acc: 64.17%
Precision, Recall and F1-Score...
                         precision    recall  f1-score   support

             低（减刑0-3个月）     0.6343    0.3542    0.4545       240
             中（减刑4-7个月）     0.6823    0.8165    0.7434      1068
            高（减刑8-12个月）     0.5637    0.4815    0.5194       432
特殊（减去余下、减为无期、减为有期、不予减刑）     0.4474    0.3290    0.3792       155

               accuracy                         0.6417      1895
              macro avg     0.5819    0.4953    0.5241      1895
           weighted avg     0.6300    0.6417    0.6259      1895

Confusion Matrix...
[[ 85 108  18  29]
 [ 39 872 132  25]
 [  0 215 208   9]
 [ 10  83  11  51]]
Time usage: 0:00:00

Process finished with exit code 0

```

## TextRCNN

```
class Config(object):

    """配置参数"""
    def __init__(self, dataset, embedding):
        self.model_name = 'TextRCNN'
        self.train_path = dataset + '/data/train.csv'                                # 训练集
        self.dev_path = dataset + '/data/dev.csv'                                    # 验证集
        self.test_path = dataset + '/data/test.csv'                                  # 测试集
        self.class_list = [x.strip() for x in open(
            dataset + '/data/class.txt', encoding='utf-8').readlines()]              # 类别名单
        self.material_path = dataset + '/material/'  # 数据材料文件夹（停用词，词典）
        self.vocab_path = dataset + '/data/vocab.pkl'                                # 词表
        self.save_path = dataset + '/saved_dict/' + self.model_name + '.ckpt'        # 模型训练结果
        self.log_path = dataset + '/log/' + self.model_name
        self.embedding_pretrained = torch.tensor(
            np.load(dataset + '/data/' + embedding)["embeddings"].astype('float32'))\
            if embedding != 'random' else None                                       # 预训练词向量
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')   # 设备

        self.dropout = 1.0                                              # 随机失活
        self.require_improvement = 1000                                 # 若超过1000batch效果还没提升，则提前结束训练
        self.num_classes = len(self.class_list)                         # 类别数
        self.n_vocab = 0                                                # 词表大小，在运行时赋值
        self.num_epochs = 100                                           # epoch数
        self.batch_size = 128                                           # mini-batch大小
        self.pad_size = 32                                              # 每句话处理成的长度(短填长切)
        self.learning_rate = 1e-3                                       # 学习率
        self.embed = self.embedding_pretrained.size(1)\
            if self.embedding_pretrained is not None else 300           # 字向量维度, 若使用了预训练词向量，则维度统一
        self.hidden_size = 256                                          # lstm隐藏层
        self.num_layers = 1                                             # lstm层数
```

```
ssh://chiyao@10.176.37.201:22/home/chiyao/anaconda3/envs/han/bin/python -u /home/chiyao/projects/Chinese-Text-Classification-Pytorch/run.py --model TextRCNN
Loading data...
Vocab size: 10002
17058it [00:02, 7198.44it/s]
1895it [00:00, 7606.42it/s]
1895it [00:00, 5359.83it/s]
Time usage: 0:00:04
/home/chiyao/anaconda3/envs/han/lib/python3.7/site-packages/torch/nn/modules/rnn.py:63: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=1.0 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
<bound method Module.parameters of Model(
  (embedding): Embedding(10002, 300, padding_idx=10001)
  (lstm): LSTM(300, 256, batch_first=True, dropout=1.0, bidirectional=True)
  (maxpool): MaxPool1d(kernel_size=32, stride=32, padding=0, dilation=1, ceil_mode=False)
  (fc): Linear(in_features=812, out_features=4, bias=True)
)>
Epoch [1/100]
Iter:      0,  Train Loss:   3.9,  Train Acc: 15.62%,  Val Loss:   2.4,  Val Acc: 12.66%,  Time: 0:00:00 * 
Iter:    100,  Train Loss:  0.83,  Train Acc: 67.19%,  Val Loss:  0.92,  Val Acc: 62.22%,  Time: 0:00:02 * 
Epoch [2/100]
Iter:    200,  Train Loss:  0.76,  Train Acc: 70.31%,  Val Loss:  0.87,  Val Acc: 64.43%,  Time: 0:00:03 * 
Epoch [3/100]
Iter:    300,  Train Loss:  0.73,  Train Acc: 71.09%,  Val Loss:  0.84,  Val Acc: 65.59%,  Time: 0:00:05 * 
Iter:    400,  Train Loss:  0.74,  Train Acc: 71.88%,  Val Loss:  0.83,  Val Acc: 66.54%,  Time: 0:00:06 * 
Epoch [4/100]
Iter:    500,  Train Loss:  0.61,  Train Acc: 72.66%,  Val Loss:  0.85,  Val Acc: 64.01%,  Time: 0:00:08  
Epoch [5/100]
Iter:    600,  Train Loss:  0.65,  Train Acc: 72.66%,  Val Loss:  0.84,  Val Acc: 64.70%,  Time: 0:00:09  
Epoch [6/100]
Iter:    700,  Train Loss:  0.61,  Train Acc: 76.56%,  Val Loss:  0.83,  Val Acc: 66.54%,  Time: 0:00:11  
Iter:    800,  Train Loss:  0.52,  Train Acc: 75.78%,  Val Loss:  0.83,  Val Acc: 66.97%,  Time: 0:00:12 * 
Epoch [7/100]
Iter:    900,  Train Loss:  0.51,  Train Acc: 83.59%,  Val Loss:  0.82,  Val Acc: 67.92%,  Time: 0:00:14 * 
Epoch [8/100]
Iter:   1000,  Train Loss:  0.56,  Train Acc: 78.91%,  Val Loss:  0.82,  Val Acc: 67.39%,  Time: 0:00:15 * 
Epoch [9/100]
Iter:   1100,  Train Loss:  0.43,  Train Acc: 88.28%,  Val Loss:  0.83,  Val Acc: 66.23%,  Time: 0:00:17  
Iter:   1200,  Train Loss:  0.37,  Train Acc: 87.50%,  Val Loss:  0.85,  Val Acc: 65.91%,  Time: 0:00:18  
Epoch [10/100]
Iter:   1300,  Train Loss:  0.34,  Train Acc: 91.41%,  Val Loss:  0.85,  Val Acc: 67.86%,  Time: 0:00:20  
Epoch [11/100]
Iter:   1400,  Train Loss:  0.27,  Train Acc: 94.53%,  Val Loss:  0.92,  Val Acc: 66.02%,  Time: 0:00:21  
Epoch [12/100]
Iter:   1500,  Train Loss:  0.31,  Train Acc: 89.84%,  Val Loss:  0.88,  Val Acc: 67.49%,  Time: 0:00:22  
Iter:   1600,  Train Loss:  0.29,  Train Acc: 92.97%,  Val Loss:  0.95,  Val Acc: 66.65%,  Time: 0:00:24  
Epoch [13/100]
Iter:   1700,  Train Loss:  0.23,  Train Acc: 96.09%,  Val Loss:  0.88,  Val Acc: 67.12%,  Time: 0:00:25  
Epoch [14/100]
Iter:   1800,  Train Loss:  0.26,  Train Acc: 92.19%,  Val Loss:  0.91,  Val Acc: 63.85%,  Time: 0:00:27  
Epoch [15/100]
Iter:   1900,  Train Loss:  0.23,  Train Acc: 92.19%,  Val Loss:  0.92,  Val Acc: 65.49%,  Time: 0:00:28  
Iter:   2000,  Train Loss:  0.15,  Train Acc: 96.88%,  Val Loss:  0.98,  Val Acc: 66.02%,  Time: 0:00:30  
No optimization for a long time, auto-stopping...
Test Loss:  0.82,  Test Acc: 67.39%
Precision, Recall and F1-Score...
                         precision    recall  f1-score   support

             低（减刑0-3个月）     0.6429    0.4500    0.5294       240
             中（减刑4-7个月）     0.6963    0.8586    0.7690      1068
            高（减刑8-12个月）     0.6322    0.4815    0.5466       432
特殊（减去余下、减为无期、减为有期、不予减刑）     0.5432    0.2839    0.3729       155

               accuracy                         0.6739      1895
              macro avg     0.6286    0.5185    0.5545      1895
           weighted avg     0.6624    0.6739    0.6556      1895

Confusion Matrix...
[[108 109  13  10]
 [ 41 917  91  19]
 [  2 214 208   8]
 [ 17  77  17  44]]
Time usage: 0:00:00

Process finished with exit code 0

```

## TextRNN_Att

```
class Config(object):

    """配置参数"""
    def __init__(self, dataset, embedding):
        self.model_name = 'TextRNN_Att'
        self.train_path = dataset + '/data/train.csv'                                # 训练集
        self.dev_path = dataset + '/data/dev.csv'                                    # 验证集
        self.test_path = dataset + '/data/test.csv'                                  # 测试集
        self.class_list = [x.strip() for x in open(
            dataset + '/data/class.txt', encoding='utf-8').readlines()]              # 类别名单
        self.material_path = dataset + '/material/'                                  # 数据材料文件夹（停用词，词典）
        self.vocab_path = dataset + '/data/vocab.pkl'                                # 词表
        self.save_path = dataset + '/saved_dict/' + self.model_name + '.ckpt'        # 模型训练结果
        self.log_path = dataset + '/log/' + self.model_name
        self.embedding_pretrained = torch.tensor(
            np.load(dataset + '/data/' + embedding)["embeddings"].astype('float32'))\
            if embedding != 'random' else None                                       # 预训练词向量
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')   # 设备

        self.dropout = 0.5                                              # 随机失活
        self.require_improvement = 1000                                 # 若超过1000batch效果还没提升，则提前结束训练
        self.num_classes = len(self.class_list)                         # 类别数
        self.n_vocab = 0                                                # 词表大小，在运行时赋值
        self.num_epochs = 100                                           # epoch数
        self.batch_size = 128                                           # mini-batch大小
        self.pad_size = 32                                              # 每句话处理成的长度(短填长切)
        self.learning_rate = 1e-3                                       # 学习率
        self.embed = self.embedding_pretrained.size(1)\
            if self.embedding_pretrained is not None else 300           # 字向量维度, 若使用了预训练词向量，则维度统一
        self.hidden_size = 128                                          # lstm隐藏层
        self.num_layers = 2                                             # lstm层数
        self.hidden_size2 = 64
```

```
ssh://chiyao@10.176.37.201:22/home/chiyao/anaconda3/envs/han/bin/python -u /home/chiyao/projects/Chinese-Text-Classification-Pytorch/run.py --model TextRNN_Att
Loading data...
Vocab size: 10002
17058it [00:02, 7301.93it/s]
1895it [00:00, 7545.79it/s]
1895it [00:00, 6058.17it/s]
Time usage: 0:00:04
<bound method Module.parameters of Model(
  (embedding): Embedding(10002, 300, padding_idx=10001)
  (lstm): LSTM(300, 128, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)
  (tanh1): Tanh()
  (tanh2): Tanh()
  (fc1): Linear(in_features=256, out_features=64, bias=True)
  (fc): Linear(in_features=64, out_features=4, bias=True)
)>
Epoch [1/100]
Iter:      0,  Train Loss:   1.4,  Train Acc: 30.47%,  Val Loss:   1.3,  Val Acc: 56.41%,  Time: 0:00:00 * 
Iter:    100,  Train Loss:  0.86,  Train Acc: 64.06%,  Val Loss:  0.92,  Val Acc: 61.00%,  Time: 0:00:03 * 
Epoch [2/100]
Iter:    200,  Train Loss:  0.83,  Train Acc: 65.62%,  Val Loss:  0.89,  Val Acc: 62.59%,  Time: 0:00:05 * 
Epoch [3/100]
Iter:    300,  Train Loss:  0.76,  Train Acc: 68.75%,  Val Loss:  0.84,  Val Acc: 65.44%,  Time: 0:00:08 * 
Iter:    400,  Train Loss:  0.84,  Train Acc: 66.41%,  Val Loss:  0.85,  Val Acc: 64.80%,  Time: 0:00:11  
Epoch [4/100]
Iter:    500,  Train Loss:   0.7,  Train Acc: 67.97%,  Val Loss:  0.86,  Val Acc: 64.12%,  Time: 0:00:13  
Epoch [5/100]
Iter:    600,  Train Loss:  0.71,  Train Acc: 71.88%,  Val Loss:  0.83,  Val Acc: 66.33%,  Time: 0:00:16 * 
Epoch [6/100]
Iter:    700,  Train Loss:  0.65,  Train Acc: 73.44%,  Val Loss:  0.84,  Val Acc: 66.28%,  Time: 0:00:19  
Iter:    800,  Train Loss:  0.54,  Train Acc: 76.56%,  Val Loss:  0.88,  Val Acc: 66.49%,  Time: 0:00:21  
Epoch [7/100]
Iter:    900,  Train Loss:  0.57,  Train Acc: 78.91%,  Val Loss:  0.93,  Val Acc: 66.70%,  Time: 0:00:24  
Epoch [8/100]
Iter:   1000,  Train Loss:  0.71,  Train Acc: 71.88%,  Val Loss:  0.92,  Val Acc: 66.91%,  Time: 0:00:26  
Epoch [9/100]
Iter:   1100,  Train Loss:  0.57,  Train Acc: 73.44%,  Val Loss:   1.0,  Val Acc: 66.07%,  Time: 0:00:29  
Iter:   1200,  Train Loss:  0.52,  Train Acc: 78.12%,  Val Loss:   1.0,  Val Acc: 65.96%,  Time: 0:00:31  
Epoch [10/100]
Iter:   1300,  Train Loss:  0.52,  Train Acc: 81.25%,  Val Loss:   1.1,  Val Acc: 66.12%,  Time: 0:00:34  
Epoch [11/100]
Iter:   1400,  Train Loss:  0.39,  Train Acc: 85.94%,  Val Loss:   1.2,  Val Acc: 63.96%,  Time: 0:00:36  
Epoch [12/100]
Iter:   1500,  Train Loss:  0.39,  Train Acc: 85.94%,  Val Loss:   1.2,  Val Acc: 64.38%,  Time: 0:00:39  
Iter:   1600,  Train Loss:  0.38,  Train Acc: 85.16%,  Val Loss:   1.3,  Val Acc: 66.12%,  Time: 0:00:41  
No optimization for a long time, auto-stopping...
Test Loss:  0.83,  Test Acc: 66.33%
Precision, Recall and F1-Score...
                         precision    recall  f1-score   support

             低（减刑0-3个月）     0.7258    0.3750    0.4945       240
             中（减刑4-7个月）     0.6921    0.8418    0.7596      1068
            高（减刑8-12个月）     0.5930    0.4722    0.5258       432
特殊（减去余下、减为无期、减为有期、不予减刑）     0.5000    0.4129    0.4523       155

               accuracy                         0.6633      1895
              macro avg     0.6277    0.5255    0.5580      1895
           weighted avg     0.6581    0.6633    0.6476      1895

Confusion Matrix...
[[ 90 125  11  14]
 [ 21 899 112  36]
 [  1 213 204  14]
 [ 12  62  17  64]]
Time usage: 0:00:00

Process finished with exit code 0

```

## DPCNN

```
class Config(object):

    """配置参数"""
    def __init__(self, dataset, embedding):
        self.model_name = 'DPCNN'
        self.train_path = dataset + '/data/train.csv'                                # 训练集
        self.dev_path = dataset + '/data/dev.csv'                                    # 验证集
        self.test_path = dataset + '/data/test.csv'                                  # 测试集
        self.class_list = [x.strip() for x in open(
            dataset + '/data/class.txt', encoding='utf-8').readlines()]              # 类别名单
        self.material_path = dataset + '/material/'                                  # 数据材料文件夹（停用词，词典）
        self.vocab_path = dataset + '/data/vocab.pkl'                                # 词表
        self.save_path = dataset + '/saved_dict/' + self.model_name + '.ckpt'        # 模型训练结果
        self.log_path = dataset + '/log/' + self.model_name
        self.embedding_pretrained = torch.tensor(
            np.load(dataset + '/data/' + embedding)["embeddings"].astype('float32'))\
            if embedding != 'random' else None                                       # 预训练词向量
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')   # 设备

        self.dropout = 0.5                                              # 随机失活
        self.require_improvement = 1000                                 # 若超过1000batch效果还没提升，则提前结束训练
        self.num_classes = len(self.class_list)                         # 类别数
        self.n_vocab = 0                                                # 词表大小，在运行时赋值
        self.num_epochs = 100                                           # epoch数
        self.batch_size = 128                                           # mini-batch大小
        self.pad_size = 32                                              # 每句话处理成的长度(短填长切)
        self.learning_rate = 1e-3                                       # 学习率
        self.embed = self.embedding_pretrained.size(1)\
            if self.embedding_pretrained is not None else 300           # 字向量维度
        self.num_filters = 250                                          # 卷积核数量(channels数)

```

```
ssh://chiyao@10.176.37.201:22/home/chiyao/anaconda3/envs/han/bin/python -u /home/chiyao/projects/Chinese-Text-Classification-Pytorch/run.py --model DPCNN
Loading data...
Vocab size: 10002
17058it [00:02, 7315.98it/s]
1895it [00:00, 7804.05it/s]
1895it [00:00, 5828.69it/s]
Time usage: 0:00:03
<bound method Module.parameters of Model(
  (embedding): Embedding(10002, 300, padding_idx=10001)
  (conv_region): Conv2d(1, 250, kernel_size=(3, 300), stride=(1, 1))
  (conv): Conv2d(250, 250, kernel_size=(3, 1), stride=(1, 1))
  (max_pool): MaxPool2d(kernel_size=(3, 1), stride=2, padding=0, dilation=1, ceil_mode=False)
  (padding1): ZeroPad2d(padding=(0, 0, 1, 1), value=0.0)
  (padding2): ZeroPad2d(padding=(0, 0, 0, 1), value=0.0)
  (relu): ReLU()
  (fc): Linear(in_features=250, out_features=4, bias=True)
)>
Epoch [1/100]
Iter:      0,  Train Loss:   1.2,  Train Acc: 35.94%,  Val Loss:   2.4,  Val Acc: 56.36%,  Time: 0:00:01 * 
Iter:    100,  Train Loss:   0.8,  Train Acc: 67.97%,  Val Loss:  0.93,  Val Acc: 62.43%,  Time: 0:00:04 * 
Epoch [2/100]
Iter:    200,  Train Loss:  0.84,  Train Acc: 69.53%,  Val Loss:  0.88,  Val Acc: 64.33%,  Time: 0:00:08 * 
Epoch [3/100]
Iter:    300,  Train Loss:  0.75,  Train Acc: 68.75%,  Val Loss:  0.85,  Val Acc: 64.80%,  Time: 0:00:12 * 
Iter:    400,  Train Loss:  0.85,  Train Acc: 61.72%,  Val Loss:  0.85,  Val Acc: 65.65%,  Time: 0:00:15  
Epoch [4/100]
Iter:    500,  Train Loss:  0.63,  Train Acc: 69.53%,  Val Loss:  0.91,  Val Acc: 61.85%,  Time: 0:00:20  
Epoch [5/100]
Iter:    600,  Train Loss:  0.64,  Train Acc: 75.00%,  Val Loss:  0.88,  Val Acc: 62.69%,  Time: 0:00:24  
Epoch [6/100]
Iter:    700,  Train Loss:  0.68,  Train Acc: 75.00%,  Val Loss:   0.9,  Val Acc: 65.22%,  Time: 0:00:28  
Iter:    800,  Train Loss:  0.42,  Train Acc: 83.59%,  Val Loss:  0.93,  Val Acc: 65.44%,  Time: 0:00:31  
Epoch [7/100]
Iter:    900,  Train Loss:  0.43,  Train Acc: 84.38%,  Val Loss:   1.0,  Val Acc: 64.33%,  Time: 0:00:36  
Epoch [8/100]
Iter:   1000,  Train Loss:   0.6,  Train Acc: 71.88%,  Val Loss:  0.98,  Val Acc: 64.54%,  Time: 0:00:40  
Epoch [9/100]
Iter:   1100,  Train Loss:  0.38,  Train Acc: 85.94%,  Val Loss:   1.1,  Val Acc: 65.54%,  Time: 0:00:44  
Iter:   1200,  Train Loss:  0.44,  Train Acc: 81.25%,  Val Loss:   1.0,  Val Acc: 62.27%,  Time: 0:00:47  
Epoch [10/100]
Iter:   1300,  Train Loss:  0.31,  Train Acc: 84.38%,  Val Loss:   1.3,  Val Acc: 62.48%,  Time: 0:00:51  
No optimization for a long time, auto-stopping...
Test Loss:  0.85,  Test Acc: 64.80%
Precision, Recall and F1-Score...
                         precision    recall  f1-score   support

             低（减刑0-3个月）     0.7636    0.3500    0.4800       240
             中（减刑4-7个月）     0.7012    0.7912    0.7435      1068
            高（减刑8-12个月）     0.5367    0.5417    0.5392       432
特殊（减去余下、减为无期、减为有期、不予减刑）     0.4514    0.4194    0.4348       155

               accuracy                         0.6480      1895
              macro avg     0.6132    0.5256    0.5494      1895
           weighted avg     0.6512    0.6480    0.6383      1895

Confusion Matrix...
[[ 84 117  17  22]
 [ 15 845 166  42]
 [  3 180 234  15]
 [  8  63  19  65]]
Time usage: 0:00:00

Process finished with exit code 0

```

## Transformer

```
class Config(object):

    """配置参数"""
    def __init__(self, dataset, embedding):
        self.model_name = 'Transformer'
        self.train_path = dataset + '/data/train.csv'                                # 训练集
        self.dev_path = dataset + '/data/dev.csv'                                    # 验证集
        self.test_path = dataset + '/data/test.csv'                                  # 测试集
        self.class_list = [x.strip() for x in open(
            dataset + '/data/class.txt', encoding='utf-8').readlines()]              # 类别名单
        self.material_path = dataset + '/material/'                                  # 数据材料文件夹（停用词，词典）
        self.vocab_path = dataset + '/data/vocab.pkl'                                # 词表
        self.save_path = dataset + '/saved_dict/' + self.model_name + '.ckpt'        # 模型训练结果
        self.log_path = dataset + '/log/' + self.model_name
        self.embedding_pretrained = torch.tensor(
            np.load(dataset + '/data/' + embedding)["embeddings"].astype('float32'))\
            if embedding != 'random' else None                                       # 预训练词向量
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')   # 设备

        self.dropout = 0.5                                              # 随机失活
        self.require_improvement = 2000                                 # 若超过1000batch效果还没提升，则提前结束训练
        self.num_classes = len(self.class_list)                         # 类别数
        self.n_vocab = 0                                                # 词表大小，在运行时赋值
        self.num_epochs = 100                                           # epoch数
        self.batch_size = 128                                           # mini-batch大小
        self.pad_size = 32                                              # 每句话处理成的长度(短填长切)
        self.learning_rate = 5e-4                                       # 学习率
        self.embed = self.embedding_pretrained.size(1)\
            if self.embedding_pretrained is not None else 300           # 字向量维度
        self.dim_model = 300
        self.hidden = 1024
        self.last_hidden = 512
        self.num_head = 5
        self.num_encoder = 2

```

```
ssh://chiyao@10.176.37.201:22/home/chiyao/anaconda3/envs/han/bin/python -u /home/chiyao/projects/Chinese-Text-Classification-Pytorch/run.py --model Transformer
Loading data...
Vocab size: 10002
17058it [00:02, 6943.48it/s]
1895it [00:00, 7521.49it/s]
1895it [00:00, 6035.98it/s]
Time usage: 0:00:04
<bound method Module.parameters of Model(
  (embedding): Embedding(10002, 300, padding_idx=10001)
  (postion_embedding): Positional_Encoding(
    (dropout): Dropout(p=0.5, inplace=False)
  )
  (encoder): Encoder(
    (attention): Multi_Head_Attention(
      (fc_Q): Linear(in_features=300, out_features=300, bias=True)
      (fc_K): Linear(in_features=300, out_features=300, bias=True)
      (fc_V): Linear(in_features=300, out_features=300, bias=True)
      (attention): Scaled_Dot_Product_Attention()
      (fc): Linear(in_features=300, out_features=300, bias=True)
      (dropout): Dropout(p=0.5, inplace=False)
      (layer_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)
    )
    (feed_forward): Position_wise_Feed_Forward(
      (fc1): Linear(in_features=300, out_features=1024, bias=True)
      (fc2): Linear(in_features=1024, out_features=300, bias=True)
      (dropout): Dropout(p=0.5, inplace=False)
      (layer_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)
    )
  )
  (encoders): ModuleList(
    (0): Encoder(
      (attention): Multi_Head_Attention(
        (fc_Q): Linear(in_features=300, out_features=300, bias=True)
        (fc_K): Linear(in_features=300, out_features=300, bias=True)
        (fc_V): Linear(in_features=300, out_features=300, bias=True)
        (attention): Scaled_Dot_Product_Attention()
        (fc): Linear(in_features=300, out_features=300, bias=True)
        (dropout): Dropout(p=0.5, inplace=False)
        (layer_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)
      )
      (feed_forward): Position_wise_Feed_Forward(
        (fc1): Linear(in_features=300, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=300, bias=True)
        (dropout): Dropout(p=0.5, inplace=False)
        (layer_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): Encoder(
      (attention): Multi_Head_Attention(
        (fc_Q): Linear(in_features=300, out_features=300, bias=True)
        (fc_K): Linear(in_features=300, out_features=300, bias=True)
        (fc_V): Linear(in_features=300, out_features=300, bias=True)
        (attention): Scaled_Dot_Product_Attention()
        (fc): Linear(in_features=300, out_features=300, bias=True)
        (dropout): Dropout(p=0.5, inplace=False)
        (layer_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)
      )
      (feed_forward): Position_wise_Feed_Forward(
        (fc1): Linear(in_features=300, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=300, bias=True)
        (dropout): Dropout(p=0.5, inplace=False)
        (layer_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (fc1): Linear(in_features=9600, out_features=4, bias=True)
)>
Epoch [1/100]
Iter:      0,  Train Loss:   1.5,  Train Acc: 21.88%,  Val Loss:   3.5,  Val Acc: 56.36%,  Time: 0:00:01 * 
Iter:    100,  Train Loss:  0.97,  Train Acc: 60.94%,  Val Loss:   1.4,  Val Acc: 57.57%,  Time: 0:00:03 * 
Epoch [2/100]
Iter:    200,  Train Loss:   1.0,  Train Acc: 57.03%,  Val Loss:   1.2,  Val Acc: 59.68%,  Time: 0:00:06 * 
Epoch [3/100]
Iter:    300,  Train Loss:  0.98,  Train Acc: 58.59%,  Val Loss:   1.1,  Val Acc: 60.53%,  Time: 0:00:09 * 
Iter:    400,  Train Loss:  0.96,  Train Acc: 62.50%,  Val Loss:   1.1,  Val Acc: 60.79%,  Time: 0:00:11  
Epoch [4/100]
Iter:    500,  Train Loss:  0.87,  Train Acc: 63.28%,  Val Loss:   1.0,  Val Acc: 57.41%,  Time: 0:00:14 * 
Epoch [5/100]
Iter:    600,  Train Loss:  0.93,  Train Acc: 56.25%,  Val Loss:   1.0,  Val Acc: 59.68%,  Time: 0:00:17 * 
Epoch [6/100]
Iter:    700,  Train Loss:  0.89,  Train Acc: 61.72%,  Val Loss:   1.1,  Val Acc: 60.47%,  Time: 0:00:20  
Iter:    800,  Train Loss:  0.88,  Train Acc: 63.28%,  Val Loss:   1.1,  Val Acc: 61.90%,  Time: 0:00:22  
Epoch [7/100]
Iter:    900,  Train Loss:  0.89,  Train Acc: 61.72%,  Val Loss:   1.0,  Val Acc: 60.95%,  Time: 0:00:25 * 
Epoch [8/100]
Iter:   1000,  Train Loss:   1.0,  Train Acc: 58.59%,  Val Loss:   1.0,  Val Acc: 59.10%,  Time: 0:00:28  
Epoch [9/100]
Iter:   1100,  Train Loss:   1.0,  Train Acc: 56.25%,  Val Loss:   1.0,  Val Acc: 62.11%,  Time: 0:00:30  
Iter:   1200,  Train Loss:  0.85,  Train Acc: 64.06%,  Val Loss:   1.0,  Val Acc: 60.95%,  Time: 0:00:33  
Epoch [10/100]
Iter:   1300,  Train Loss:  0.75,  Train Acc: 72.66%,  Val Loss:   1.0,  Val Acc: 62.16%,  Time: 0:00:36  
Epoch [11/100]
Iter:   1400,  Train Loss:  0.92,  Train Acc: 59.38%,  Val Loss:   1.1,  Val Acc: 61.79%,  Time: 0:00:38  
Epoch [12/100]
Iter:   1500,  Train Loss:  0.92,  Train Acc: 60.94%,  Val Loss:   1.0,  Val Acc: 62.11%,  Time: 0:00:41 * 
Iter:   1600,  Train Loss:  0.73,  Train Acc: 69.53%,  Val Loss:   1.0,  Val Acc: 61.85%,  Time: 0:00:44  
Epoch [13/100]
Iter:   1700,  Train Loss:  0.85,  Train Acc: 64.84%,  Val Loss:   1.0,  Val Acc: 62.27%,  Time: 0:00:46  
Epoch [14/100]
Iter:   1800,  Train Loss:  0.84,  Train Acc: 68.75%,  Val Loss:   1.0,  Val Acc: 61.00%,  Time: 0:00:49  
Epoch [15/100]
Iter:   1900,  Train Loss:  0.75,  Train Acc: 71.09%,  Val Loss:   1.0,  Val Acc: 61.90%,  Time: 0:00:52  
Iter:   2000,  Train Loss:   0.8,  Train Acc: 67.97%,  Val Loss:   1.0,  Val Acc: 61.79%,  Time: 0:00:54  
Epoch [16/100]
Iter:   2100,  Train Loss:  0.72,  Train Acc: 69.53%,  Val Loss:   1.0,  Val Acc: 62.11%,  Time: 0:00:57  
Epoch [17/100]
Iter:   2200,  Train Loss:  0.76,  Train Acc: 68.75%,  Val Loss:   1.0,  Val Acc: 60.79%,  Time: 0:01:00  
Epoch [18/100]
Iter:   2300,  Train Loss:  0.71,  Train Acc: 64.84%,  Val Loss:   1.0,  Val Acc: 61.90%,  Time: 0:01:03  
Iter:   2400,  Train Loss:  0.56,  Train Acc: 80.47%,  Val Loss:   1.0,  Val Acc: 61.79%,  Time: 0:01:05  
Epoch [19/100]
Iter:   2500,  Train Loss:  0.79,  Train Acc: 64.06%,  Val Loss:   1.0,  Val Acc: 61.58%,  Time: 0:01:08  
Epoch [20/100]
Iter:   2600,  Train Loss:  0.74,  Train Acc: 71.88%,  Val Loss:   1.0,  Val Acc: 60.95%,  Time: 0:01:11  
Epoch [21/100]
Iter:   2700,  Train Loss:  0.87,  Train Acc: 67.97%,  Val Loss:   1.0,  Val Acc: 61.85%,  Time: 0:01:13  
Iter:   2800,  Train Loss:  0.72,  Train Acc: 69.53%,  Val Loss:   1.0,  Val Acc: 61.58%,  Time: 0:01:16  
Epoch [22/100]
Iter:   2900,  Train Loss:   0.8,  Train Acc: 65.62%,  Val Loss:   1.0,  Val Acc: 62.11%,  Time: 0:01:19  
Epoch [23/100]
Iter:   3000,  Train Loss:  0.78,  Train Acc: 62.50%,  Val Loss:   1.0,  Val Acc: 61.42%,  Time: 0:01:21  
Epoch [24/100]
Iter:   3100,  Train Loss:  0.65,  Train Acc: 74.22%,  Val Loss:   1.0,  Val Acc: 62.06%,  Time: 0:01:24  
Iter:   3200,  Train Loss:  0.74,  Train Acc: 67.97%,  Val Loss:   1.0,  Val Acc: 61.64%,  Time: 0:01:27  
Epoch [25/100]
Iter:   3300,  Train Loss:  0.71,  Train Acc: 71.88%,  Val Loss:   1.0,  Val Acc: 62.22%,  Time: 0:01:29  
Epoch [26/100]
Iter:   3400,  Train Loss:  0.83,  Train Acc: 66.41%,  Val Loss:   1.0,  Val Acc: 62.11%,  Time: 0:01:32  
Epoch [27/100]
Iter:   3500,  Train Loss:  0.74,  Train Acc: 71.09%,  Val Loss:   1.0,  Val Acc: 62.11%,  Time: 0:01:35  
No optimization for a long time, auto-stopping...
Test Loss:   1.0,  Test Acc: 62.11%
Precision, Recall and F1-Score...
                         precision    recall  f1-score   support

             低（减刑0-3个月）     0.5638    0.3500    0.4319       240
             中（减刑4-7个月）     0.6672    0.7940    0.7251      1068
            高（减刑8-12个月）     0.5500    0.4329    0.4845       432
特殊（减去余下、减为无期、减为有期、不予减刑）     0.4296    0.3742    0.4000       155

               accuracy                         0.6211      1895
              macro avg     0.5526    0.4878    0.5104      1895
           weighted avg     0.6079    0.6211    0.6065      1895

Confusion Matrix...
[[ 84 120  13  23]
 [ 50 848 124  46]
 [  7 230 187   8]
 [  8  73  16  58]]
Time usage: 0:00:00

Process finished with exit code 0

```

